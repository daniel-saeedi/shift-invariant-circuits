# Circuit Shift-Invariance in Semantic Processing

**Investigating whether semantic circuits maintain consistent activation patterns across different contextual positions**

---

## Abstract

This research explores **shift-invariance** in transformer neural circuits - specifically whether semantic features that detect concepts like "Texas" maintain their activation patterns when the same content appears at different token positions or with varying contextual information.

## Research Question

> *Do circuit features that detect specific semantic concepts maintain their activation patterns when the same semantic content appears at different token positions in varied contexts?*

---

## Methodology

We analyze the "Texas" supernode from a circuit that processes geographical knowledge about Dallas and Austin. The supernode consists of 6 features across layers 4-20 that collectively represent the concept of Texas.

**Test Cases:**
1. **Position shift with unrelated context** - Adding irrelevant narrative text
2. **Position shift with related context** - Adding Texas-relevant historical information  
3. **Context with explicit mentions** - Including direct references to Texas

---

## Key Findings

### üîç **Finding 1: Near-Perfect Shift Invariance with Unrelated Context**

When we prepend completely unrelated context (sunset, trees, crickets), the Texas circuit **moves perfectly** to the new position:

```json
Activation ratios (new/old): [0.98, 1.03, 0.99, 0.98, 0.98, 0.91]
```
**Interpretation:** Ratios ‚âà 1.0 indicate the circuit has successfully shifted to process the same semantic content at the new position.

### üîç **Finding 2: Partial Shift with Related Context**

With Texas-relevant historical context, the circuit shows **mixed behavior**:

```json
Activation ratios: [0.88, 1.00, 0.85, 0.86, 0.77, 1.09]
```
**Interpretation:** Some features maintain strength while others are partially activated by the earlier context.

### üîç **Finding 3: Circuit Distribution with Explicit Mentions**

When Texas is explicitly mentioned in context, the circuit shows **distributed activation**:

```json
Activation ratios: [0.58, 0.79, 0.81, 0.62, 0.61, 0.80]
```
**Interpretation:** The circuit forms earlier in the sequence, reducing activation at the target position.

---
